# MedMamba 2D → 1D ECG 适配修改计划

## 目标
将 MedMamba.py 从 2D 图像分类改造为 1D ECG 信号分类（输入：12×5000）

---

## 修改清单

### ✅ 准备工作
- [ ] 备份原始文件
- [ ] 创建测试脚本 `test_1d_modules.py`
- [ ] 准备测试数据：`torch.randn(2, 12, 5000)`

---

### 步骤1: PatchEmbed1D（第26-44行）

**文件位置**: `class PatchEmbed2D` → 新建 `class PatchEmbed1D`

**修改内容**:
- [ ] 复制 `PatchEmbed2D` 为 `PatchEmbed1D`
- [ ] `nn.Conv2d` → `nn.Conv1d`
- [ ] 删除 `patch_size` 的 tuple 处理（保持 int）
- [ ] 调整 `forward` 输出：`(B, L', D)` 格式
- [ ] 删除 `permute(0, 2, 3, 1)`，改为 `permute(0, 2, 1)`

**测试代码**:
```python
embed = PatchEmbed1D(patch_size=25, in_chans=12, embed_dim=96)
x = torch.randn(2, 12, 5000)
out = embed(x)  # 期望: (2, 200, 96)
```

---

### 步骤2: PatchMerging1D（第47-91行）

**文件位置**: `class PatchMerging2D` → 新建 `class PatchMerging1D`

**修改内容**:
- [ ] 复制 `PatchMerging2D` 为 `PatchMerging1D`
- [ ] 删除 H/W 处理，只保留 L
- [ ] `input shape`: `(B, L, C)` → `(B, L//2, 2*C)`
- [ ] 删除 SHAPE_FIX 逻辑（简化）
- [ ] `nn.Linear(2 * dim, 2 * dim)` 实现下采样

**测试代码**:
```python
merge = PatchMerging1D(dim=96)
x = torch.randn(2, 200, 96)
out = merge(x)  # 期望: (2, 100, 192)
```

---

### 步骤3: SS1D - 核心状态空间模型（第94-261行）⚠️ 最关键

**文件位置**: `class SS2D` → 新建 `class SS1D`

**策略选择**: 采用**时间双向扫描（K=2）**，即正向和反向两个方向

---

#### 3.1 `__init__` 方法修改

##### 修改点1: 卷积层改为1D（第109-117行）
```python
# 原代码（2D）：
self.conv2d = nn.Conv2d(
    in_channels=self.d_inner,
    out_channels=self.d_inner,
    groups=self.d_inner,
    bias=conv_bias,
    kernel_size=d_conv,
    padding=(d_conv - 1) // 2,
    **factory_kwargs,
)

# 修改为（1D）：
self.conv1d = nn.Conv1d(
    in_channels=self.d_inner,
    out_channels=self.d_inner,
    groups=self.d_inner,  # Depthwise Conv
    bias=conv_bias,
    kernel_size=d_conv,   # 保持为3或5
    padding=(d_conv - 1) // 2,
    **factory_kwargs,
)
```
**注意**: 变量名从 `conv2d` 改为 `conv1d`

##### 修改点2: x_proj 从4个方向减少为2个（第120-127行）
```python
# 原代码（K=4）：
self.x_proj = (
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
)
self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)

# 修改为（K=2）：
self.x_proj = (
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
    nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
)
self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=2, N, inner)
```

##### 修改点3: dt_projs 从4个减少为2个（第129-137行）
```python
# 原代码（K=4）：
self.dt_projs = (
    self.dt_init(...),
    self.dt_init(...),
    self.dt_init(...),
    self.dt_init(...),
)
self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)
self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)

# 修改为（K=2）：
self.dt_projs = (
    self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
    self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
)
self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=2, inner, rank)
self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=2, inner)
```

##### 修改点4: A_logs 和 Ds 的 copies 参数（第139-140行）
```python
# 原代码：
self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)
self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)

# 修改为：
self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=2, merge=True) # (K=2, D, N)
self.Ds = self.D_init(self.d_inner, copies=2, merge=True) # (K=2, D, N)
```

**checklist**:
- [ ] 完成修改点1（Conv2d→Conv1d）
- [ ] 完成修改点2（x_proj K=4→2）
- [ ] 完成修改点3（dt_projs K=4→2）
- [ ] 完成修改点4（A_logs, Ds copies=2）

---

#### 3.2 `forward_corev0` 方法修改（第205-242行）

这是**最核心**的修改！

##### 修改点1: 输入形状和K值（第207-210行）
```python
# 原代码（2D）：
B, C, H, W = x.shape
L = H * W
K = 4

# 修改为（1D）：
B, C, L = x.shape  # 输入是 (B, d_inner, 5000)
K = 2
```

##### 修改点2: 扫描序列生成（第212-213行）⚠️ 关键！
```python
# 原代码（2D，4方向）：
x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)
xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k=4, d, l)

# 修改为（1D，2方向）：
xs = torch.stack([
    x,                        # 时间正向：(B, C, L)
    torch.flip(x, dims=[-1])  # 时间反向：(B, C, L)
], dim=1)  # (B, K=2, C, L)
```
**说明**: 直接对时间维度（最后一维）做正向和反向扫描

##### 修改点3: einsum 操作保持不变（第215-218行）
```python
# K从4变为2，但einsum逻辑相同：
x_dbl = torch.einsum("b k d l, k c d -> b k c l", xs.view(B, K, -1, L), self.x_proj_weight)
dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)
dts = torch.einsum("b k r l, k d r -> b k d l", dts.view(B, K, -1, L), self.dt_projs_weight)
```
**注意**: 这部分不需要改，因为K维度自动适配

##### 修改点4: selective_scan 调用保持不变（第221-236行）
```python
# 这部分逻辑完全相同，只是K=2
xs = xs.float().view(B, -1, L) # (b, k * d, l)  -> (B, 2*d_inner, L)
dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)
Bs = Bs.float().view(B, K, -1, L) # (b, k=2, d_state, l)
Cs = Cs.float().view(B, K, -1, L) # (b, k=2, d_state, l)
Ds = self.Ds.float().view(-1) # (k * d)
As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)
dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)

out_y = self.selective_scan(
    xs, dts, 
    As, Bs, Cs, Ds, z=None,
    delta_bias=dt_projs_bias,
    delta_softplus=True,
    return_last_state=False,
).view(B, K, -1, L)  # (B, 2, d_inner, L)
assert out_y.dtype == torch.float
```

##### 修改点5: 输出处理简化（第238-242行）
```python
# 原代码（2D，4方向）：
inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)
wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y

# 修改为（1D，2方向）：
# out_y[:, 0] 是时间正向的输出
# out_y[:, 1] 是时间反向的输出（已经反向扫描过，需要再翻转回来）
inv_y = torch.flip(out_y[:, 1:2], dims=[-1])  # 把反向结果翻转回正常顺序
return out_y[:, 0], inv_y[:, 0]  # 只返回2个tensor
```

**checklist**:
- [ ] 修改输入shape解析（B,C,L）
- [ ] 修改K=2
- [ ] 重写扫描序列生成（关键！）
- [ ] 简化输出为2个方向
- [ ] 删除所有H/W相关的transpose操作

---

#### 3.3 `forward` 方法修改（第244-261行）

##### 修改点1: 输入形状（第245行）
```python
# 原代码（2D）：
B, H, W, C = x.shape

# 修改为（1D）：
B, L, C = x.shape  # 输入是 (B, 200, 96)
```

##### 修改点2: 维度变换（第247-251行）
```python
# 原代码（2D）：
xz = self.in_proj(x)
x, z = xz.chunk(2, dim=-1) # (b, h, w, d)
x = x.permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)
x = self.act(self.conv2d(x)) # (b, d, h, w)

# 修改为（1D）：
xz = self.in_proj(x)  # (B, L, 2*d_inner)
x, z = xz.chunk(2, dim=-1)  # 每个是 (B, L, d_inner)
x = x.permute(0, 2, 1).contiguous()  # (B, d_inner, L)
x = self.act(self.conv1d(x))  # (B, d_inner, L)
```

##### 修改点3: forward_core 调用和结果融合（第252-254行）
```python
# 原代码（2D，4方向）：
y1, y2, y3, y4 = self.forward_core(x)
assert y1.dtype == torch.float32
y = y1 + y2 + y3 + y4

# 修改为（1D，2方向）：
y1, y2 = self.forward_core(x)  # 只有2个方向
assert y1.dtype == torch.float32
y = y1 + y2  # 简单相加融合
```

##### 修改点4: 输出维度变换（第255-261行）
```python
# 原代码（2D）：
y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)
y = self.out_norm(y)
y = y * F.silu(z)
out = self.out_proj(y)
if self.dropout is not None:
    out = self.dropout(out)
return out

# 修改为（1D）：
y = y.permute(0, 2, 1).contiguous()  # (B, d_inner, L) -> (B, L, d_inner)
y = self.out_norm(y)  # (B, L, d_inner)
y = y * F.silu(z)  # 门控机制
out = self.out_proj(y)  # (B, L, d_model)
if self.dropout is not None:
    out = self.dropout(out)
return out
```

**checklist**:
- [ ] 修改输入shape为(B,L,C)
- [ ] permute改为(0,2,1)
- [ ] conv2d改为conv1d
- [ ] forward_core返回值改为2个
- [ ] y=y1+y2（不是y1+y2+y3+y4）
- [ ] 输出permute改为(0,2,1)

---

#### 3.4 完整的修改后代码框架

```python
class SS1D(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=3, expand=2, ...):
        super().__init__()
        # ... 参数初始化 ...
        
        # 改为Conv1d
        self.conv1d = nn.Conv1d(self.d_inner, self.d_inner, ...)
        
        # K=2的投影矩阵
        self.x_proj = (nn.Linear(...), nn.Linear(...))  # 2个
        self.dt_projs = (self.dt_init(...), self.dt_init(...))  # 2个
        
        # copies=2
        self.A_logs = self.A_log_init(..., copies=2, merge=True)
        self.Ds = self.D_init(..., copies=2, merge=True)
        
    def forward_corev0(self, x: torch.Tensor):
        B, C, L = x.shape  # 1D输入
        K = 2
        
        # 简化的2方向扫描
        xs = torch.stack([x, torch.flip(x, dims=[-1])], dim=1)
        
        # ... selective_scan处理 ...
        
        # 只返回2个方向
        inv_y = torch.flip(out_y[:, 1:2], dims=[-1])
        return out_y[:, 0], inv_y[:, 0]
    
    def forward(self, x: torch.Tensor, **kwargs):
        B, L, C = x.shape  # (B, 200, 96)
        
        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1)
        
        x = x.permute(0, 2, 1).contiguous()  # (B, C, L)
        x = self.act(self.conv1d(x))
        
        y1, y2 = self.forward_core(x)  # 2个方向
        y = y1 + y2
        
        y = y.permute(0, 2, 1).contiguous()  # (B, L, C)
        y = self.out_norm(y)
        y = y * F.silu(z)
        out = self.out_proj(y)
        
        if self.dropout is not None:
            out = self.dropout(out)
        return out
```

---

**测试代码**:
```python
# 单元测试
ss1d = SS1D(d_model=96, d_state=16, d_conv=3, expand=2)
x = torch.randn(2, 200, 96)  # (batch, length, channels)
out = ss1d(x)
print(out.shape)  # 期望: torch.Size([2, 200, 96])

# 梯度测试
out.sum().backward()
print("Gradient test passed!")
```

---

### 步骤4: SS_Conv_SSM1D（第286-331行）

**文件位置**: `class SS_Conv_SSM` → 修改为支持1D

**修改内容**:
- [ ] `SS2D` → `SS1D` 调用
- [ ] `conv33conv33conv11` 序列改为1D卷积
  - [ ] `nn.Conv2d` → `nn.Conv1d`
  - [ ] `nn.BatchNorm2d` → `nn.BatchNorm1d`
  - [ ] kernel_size: (3,3) → 3
- [ ] 删除 `permute(0,3,1,2)` 和 `permute(0,2,3,1)`
- [ ] 改为 `permute(0,2,1)` 和 `permute(0,2,1)`
- [ ] 调整 `chunk` 和 `cat` 的维度

**测试代码**:
```python
block = SS_Conv_SSM(hidden_dim=96, d_state=16)
x = torch.randn(2, 200, 96)
out = block(x)  # 期望: (2, 200, 96)
```

---

### 步骤5: VSSLayer（第334-399行）

**文件位置**: `class VSSLayer` → 基本不变

**修改内容**:
- [ ] 确认 `SS_Conv_SSM` 已适配1D
- [ ] 将 `downsample=PatchMerging2D` → `PatchMerging1D`

**测试代码**:
```python
layer = VSSLayer(dim=96, depth=2, downsample=PatchMerging1D)
x = torch.randn(2, 200, 96)
out = layer(x)  # 期望: (2, 100, 192) - 经过downsample
```

---

### 步骤6: VSSM 主网络（第402-496行）

**文件位置**: `class VSSM` → 修改初始化和forward

#### 6.1 `__init__` 修改
- [ ] `PatchEmbed2D` → `PatchEmbed1D`
- [ ] 删除 `absolute_pos_embed` 的2D逻辑
- [ ] `PatchMerging2D` → `PatchMerging1D`
- [ ] 确认 `dims` 参数适配

#### 6.2 `forward` 修改
- [ ] `x.permute(0,3,1,2)` → `x.permute(0,2,1)`
- [ ] `nn.AdaptiveAvgPool2d(1)` → `nn.AdaptiveAvgPool1d(1)`
- [ ] 确认 `flatten` 维度

**测试代码**:
```python
model = VSSM(
    patch_size=25,
    in_chans=12,
    num_classes=6,
    depths=[2, 2, 4, 2],
    dims=[96, 192, 384, 768]
)
x = torch.randn(2, 12, 5000)
out = model(x)  # 期望: (2, 6)
```

---

### 步骤7: 更新模型实例（第499-505行）

**文件位置**: 文件末尾的模型定义

**修改内容**:
- [ ] 更新 `medmamba_t` 参数
- [ ] 更新 `medmamba_s` 参数
- [ ] 更新 `medmamba_b` 参数
- [ ] 调整测试数据形状

```python
medmamba_t = VSSM(
    patch_size=25, 
    in_chans=12, 
    depths=[2, 2, 4, 2],
    dims=[96, 192, 384, 768],
    num_classes=6
).to("cuda")

data = torch.randn(1, 12, 5000).to("cuda")
print(medmamba_t(data).shape)  # torch.Size([1, 6])
```

---

## 验收标准

### 单元测试
- [ ] PatchEmbed1D 输出形状正确
- [ ] PatchMerging1D 输出形状正确
- [ ] SS1D 输出形状正确
- [ ] SS_Conv_SSM 输出形状正确
- [ ] VSSLayer 输出形状正确
- [ ] VSSM 完整前向传播无错误

### 集成测试
- [ ] 输入 (B, 12, 5000) → 输出 (B, 6)
- [ ] 梯度可以正常反向传播
- [ ] 无 CUDA 错误
- [ ] 可以加载预训练权重（如有）

### 性能测试
- [ ] 计算 FLOPs
- [ ] 测试推理速度
- [ ] 测试显存占用

---

## 注意事项

⚠️ **关键点**:
1. 维度对应：`(B,H,W,C)` → `(B,L,C)`
2. 扫描方向：4方向 → 2方向（正向+反向）
3. 所有 `Conv2d` → `Conv1d`
4. 所有 `BatchNorm2d` → `BatchNorm1d`
5. 仔细检查 `permute` 的维度索引

⚠️ **常见错误**:
- 忘记修改 `permute` 的维度参数
- `view` 和 `reshape` 的形状不匹配
- `transpose` 的维度索引错误

---



输入: (B, 12, 5000)
    ↓
[1] PatchEmbed1D (patch_size=5)
    ├─ Conv1d(12 → 96, kernel=5, stride=5)
    └─ permute
    ↓
(B, 1000, 96)  # 1000个时间patch，96维特征
    ↓
[2] Position Dropout
    ↓
┌─────────────── Stage 1 ─────────────┐
│ dim=96, depth=2                      │
│   ├─ SS_Conv_SSM Block 1             │
│   │    ├─ 左半通道(48): Conv分支     │
│   │    ├─ 右半通道(48): SS1D(双向SSM)│
│   │    └─ ChannelShuffle + 残差     │
│   └─ SS_Conv_SSM Block 2             │
│   └─ PatchMerging1D                  │
│        └─ (B, 1000, 96) → (B, 500, 192) │
└──────────────────────────────────────┘
    ↓
┌─────────────── Stage 2 ─────────────┐
│ dim=192, depth=2                     │
│   ├─ SS_Conv_SSM Block 1             │
│   └─ SS_Conv_SSM Block 2             │
│   └─ PatchMerging1D                  │
│        └─ (B, 500, 192) → (B, 250, 384)│
└──────────────────────────────────────┘
    ↓
┌─────────────── Stage 3 ─────────────┐
│ dim=384, depth=8 (最深)              │
│   ├─ SS_Conv_SSM Block 1-8           │
│   └─ PatchMerging1D                  │
│        └─ (B, 250, 384) → (B, 125, 768)│
└──────────────────────────────────────┘
    ↓
┌─────────────── Stage 4 ─────────────┐
│ dim=768, depth=2                     │
│   ├─ SS_Conv_SSM Block 1             │
│   └─ SS_Conv_SSM Block 2             │
│   └─ 无PatchMerging                  │
└──────────────────────────────────────┘
    ↓
(B, 125, 768)
    ↓
[3] Permute + AdaptiveAvgPool1d
    └─ (B, 768, 125) → (B, 768, 1)
    ↓
[4] Flatten
    └─ (B, 768)
    ↓
[5] Linear Head (768 → num_classes)
    ↓
输出: (B, num_classes)