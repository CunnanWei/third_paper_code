{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wfdb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as snss\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CSV: datasets/pretrain/train.csv\n",
      "Val CSV: datasets/pretrain/val.csv\n"
     ]
    }
   ],
   "source": [
    "# 检查train和val的csv文件\n",
    "train_csv_path = 'datasets/pretrain/train.csv'\n",
    "val_csv_path = 'datasets/pretrain/val.csv'\n",
    "\n",
    "print(f'Train CSV: {train_csv_path}')\n",
    "print(f'Val CSV: {val_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_csv_path, low_memory=False)\n",
    "ecg = np.load(data['path'][0])\n",
    "# 将 numpy 数组转换为 DataFrame 后再保存\n",
    "pd.DataFrame(ecg).to_csv('ecg.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subject_id', 'study_id', 'cart_id', 'ecg_time', 'report_0', 'report_1',\n",
       "       'report_2', 'report_3', 'report_4', 'report_5', 'report_6', 'report_7',\n",
       "       'report_8', 'report_9', 'report_10', 'report_11', 'report_12',\n",
       "       'report_13', 'report_14', 'report_15', 'report_16', 'report_17',\n",
       "       'bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end',\n",
       "       'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', 't_axis',\n",
       "       'report_length', 'total_report', 'path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先查看train csv的列\n",
    "train_df = pd.read_csv(train_csv_path, low_memory=False)\n",
    "print('Train CSV columns:')\n",
    "print(train_df.columns.tolist())\n",
    "print(f'\\nTrain rows: {len(train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atrial fibrillation with rapid ventricular response.. cannot rule out anteroseptal infarct - age undetermined. possible left ventricular hypertrophy. inferior/lateral st-t changes are probably due to ventricular hypertrophy. abnormal ecg.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第一条report示例\n",
    "print('Train report示例:')\n",
    "print(train_df['total_report'][0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "开始处理 train 数据集\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train: 100%|██████████| 745405/745405 [06:19<00:00, 1961.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 完成！处理了 745405 条记录\n",
      "\n",
      "================================================================================\n",
      "开始处理 val 数据集\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing val: 100%|██████████| 15213/15213 [00:08<00:00, 1863.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 完成！处理了 15213 条记录\n",
      "\n",
      "================================================================================\n",
      "全部完成！\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 text_model='ncbi/MedCPT-Query-Encoder',\n",
    "                 free_layers=6,           # 冻结前6层\n",
    "                 proj_hidden=256,         # 投影层隐藏维度\n",
    "                 proj_out=256):           # 投影层输出维度\n",
    "        super().__init__()\n",
    "        \n",
    "        # ========== 1. Text Encoder ==========\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            text_model             \n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, text, device=None):\n",
    "        tokenizer_output = self.tokenizer.batch_encode_plus(\n",
    "            batch_text_or_text_pairs=text,\n",
    "            add_special_tokens=True,    \n",
    "            truncation=True,             \n",
    "            max_length=256,              \n",
    "            padding='max_length',        \n",
    "            return_tensors='pt'          \n",
    "        )\n",
    "        return tokenizer_output\n",
    "    \n",
    "    def forward(self, report):\n",
    "        return self._tokenize(report)\n",
    "\n",
    "text_encoder = TextEncoder()\n",
    "\n",
    "# 创建统一的tokenize结果目录\n",
    "tokenize_dir = Path('datasets/pretrain/report_tokenize')\n",
    "tokenize_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 处理train和val两个数据集\n",
    "datasets = ['train', 'val']\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'开始处理 {dataset_name} 数据集')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    # 读取csv\n",
    "    csv_path = f'datasets/pretrain/{dataset_name}.csv'\n",
    "    df = pd.read_csv(csv_path, low_memory=False)\n",
    "    \n",
    "    # 在csv中添加新列（如果不存在）\n",
    "    if 'report_tokenize_path' not in df.columns:\n",
    "        df['report_tokenize_path'] = ''\n",
    "    \n",
    "    # 逐行处理每个report\n",
    "    for i in tqdm(range(len(df)), desc=f'Tokenizing {dataset_name}'):\n",
    "        # 获取当前行的report\n",
    "        report_text = df.loc[i, 'total_report']\n",
    "        \n",
    "        # tokenize (需要包装成列表)\n",
    "        tokenized = text_encoder._tokenize([report_text])\n",
    "        \n",
    "        # 生成npz文件路径 (格式: train_000000.npz 或 val_000000.npz)\n",
    "        npz_filename = f'{i:06d}.npz'\n",
    "        npz_path = tokenize_dir / npz_filename\n",
    "        # 保存tokenized结果到npz (取第一个元素，因为batch size=1)\n",
    "        np.savez(\n",
    "            npz_path,\n",
    "            input_ids=tokenized['input_ids'][0].numpy(),\n",
    "            attention_mask=tokenized['attention_mask'][0].numpy()\n",
    "        )\n",
    "        \n",
    "        # 在csv中记录相对路径\n",
    "        relative_path = f'datasets/pretrain/report_tokenize/{npz_filename}'\n",
    "        df.loc[i, 'report_tokenize_path'] = relative_path\n",
    "    \n",
    "    # 保存更新后的csv\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'{dataset_name} 完成！处理了 {len(df)} 条记录')\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print('全部完成！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证：读取train和val各一个npz文件查看内容\n",
    "for dataset_name in ['train', 'val']:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'{dataset_name.upper()} 数据集验证')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # 读取csv\n",
    "    df = pd.read_csv(f'datasets/pretrain/{dataset_name}.csv', low_memory=False)\n",
    "    \n",
    "    test_idx = 0\n",
    "    test_path = df.loc[test_idx, 'report_tokenize_path']\n",
    "    print(f'NPZ路径: {test_path}')\n",
    "    \n",
    "    # 加载npz文件\n",
    "    loaded = np.load(test_path)\n",
    "    print(f'\\nNPZ包含的数组:')\n",
    "    for key in loaded.files:\n",
    "        print(f'  {key}: shape={loaded[key].shape}, dtype={loaded[key].dtype}')\n",
    "    \n",
    "    print(f'\\ninput_ids前10个token: {loaded[\"input_ids\"][0][:10]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证配对关系：检查train和val各前2行数据\n",
    "for dataset_name in ['train', 'val']:\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'{dataset_name.upper()} 验证配对关系（前2行）:')\n",
    "    print('='*80)\n",
    "    \n",
    "    df = pd.read_csv(f'datasets/pretrain/{dataset_name}.csv', low_memory=False)\n",
    "    \n",
    "    for i in range(min(2, len(df))):\n",
    "        print(f'\\n第{i}行:')\n",
    "        print(f'  Report片段: {df.loc[i, \"total_report\"][:50]}...')\n",
    "        print(f'  Tokenize路径: {df.loc[i, \"report_tokenize_path\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看更新后的csv结构统计\n",
    "print('处理结果统计:')\n",
    "print('='*80)\n",
    "\n",
    "for dataset_name in ['train', 'val']:\n",
    "    df = pd.read_csv(f'datasets/pretrain/{dataset_name}.csv', low_memory=False)\n",
    "    \n",
    "    print(f'\\n{dataset_name.upper()} 数据集:')\n",
    "    print(f'  列名: {df.columns.tolist()}')\n",
    "    print(f'  总行数: {len(df)}')\n",
    "    print(f'  report_tokenize_path列非空数: {df[\"report_tokenize_path\"].notna().sum()}')\n",
    "    \n",
    "    # 检查是否有空值\n",
    "    if df[\"report_tokenize_path\"].isna().sum() > 0:\n",
    "        print(f'  ⚠️  警告：有 {df[\"report_tokenize_path\"].isna().sum()} 行路径为空')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('✓ 所有数据处理完成！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看tokenize目录内容\n",
    "import os\n",
    "tokenize_dir = 'datasets/pretrain/report_tokenize'\n",
    "if os.path.exists(tokenize_dir):\n",
    "    files = os.listdir(tokenize_dir)\n",
    "    train_files = [f for f in files if f.startswith('train_')]\n",
    "    val_files = [f for f in files if f.startswith('val_')]\n",
    "    \n",
    "    print(f'Tokenize目录: {tokenize_dir}')\n",
    "    print(f'  Train文件数: {len(train_files)}')\n",
    "    print(f'  Val文件数: {len(val_files)}')\n",
    "    print(f'  总文件数: {len(files)}')\n",
    "    \n",
    "    if len(train_files) > 0:\n",
    "        print(f'\\n示例文件名:')\n",
    "        print(f'  Train: {train_files[0]}')\n",
    "        if len(val_files) > 0:\n",
    "            print(f'  Val: {val_files[0]}')\n",
    "else:\n",
    "    print(f'目录不存在: {tokenize_dir}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
